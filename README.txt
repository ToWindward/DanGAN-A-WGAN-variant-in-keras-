Hey all. I created this WGAN variant when I was playing around with classic and gradient penalty Wasserstein GANs and thought I would put it on here in case anyone was interested or felt like coming up with improvements. It's currently called the DanGAN because that's my name and I haven't come up with a fun acronym yet so I'm open to suggestions.

The basic premise is that instead of clipping the weights (as in classic WGAN) or penalising the gradients, instead it uses an adaptive learning rate for the discriminator/critic which prevents exploding gradients. For the generator instead of trying to maximise the output, the loss function attempts to match the mean(squared) and variance of the prior real outputs from the discriminator/critic. Matching to the mean allows the generator to be trained for multiple iterations without the generator collapsing, while matching the variance helps to combat mode collapse which was a significant issue in earlier versions. This GAN is somewhat sensitive to hyperparameters, as well as the priority given to the mean vs the variance. It works well with small networks but has some issues with larger networks. As a methodolgy this doesn't really compare with current state of the art methods and is more comparable with WGAN / WGAN-GP.

Everything is written in python, using GPU keras and the functional API. While you should be able to take the code, run it, and get results, this is a work in progress, so bear in mind it might have quirks. You also need to find some images to train it with, this version is written for 128x128x3 images, but you can modify it pretty easily for other image sizes.

There are quite a lot of hyperparameters and I'm still working on finding the best ones. The ones in the code should work pretty well but if anyone finds better ones then do let me know. Same goes for network architecture, kernel size etc. I've played around quite a bit with batch norm and regularization layers, these are included where they seem to help but again, if anyone gets decent results with alternatives I'd love to know. With regards to optimizers it currently uses RMSprop and I didn't have good results with Adam but I haven't tested many others.

P.S. There should be 2 files, one for the GAN itself as well as an example execution file. I've also included a folder with some example pictures.
